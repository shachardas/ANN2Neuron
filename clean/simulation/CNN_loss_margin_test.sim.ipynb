{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import modules.model2sim as sim\n",
    "\n",
    "import torch\n",
	"import torch.nn as nn\n",
	"import numpy as np\n",
	"\n",
	"class customLoss(nn.Module):\n",
	"    def __init__(self, p=0.1):\n",
	"        super().__init__()\n",
	"        self.p = p\n",
	"        self.cost = nn.CrossEntropyLoss()\n",
	"        self.activation = nn.LogSoftmax()\n",
	"\n",
	"    def forward(self, output, labels):\n",
	"        tempOut = torch.clone(output)\n",
	"        \n",
	"        for i in range(labels.size(0)):\n",
	"            tempOut[i,labels[i]] = -1 * tempOut[i,labels[i]]\n",
	"\n",
	"        tempOut = torch.where(tempOut>-5,tempOut+10,torch.zeros_like(tempOut))\n",
	"\n",
	"        return self.p    * torch.mean(tempOut) \\\n",
	"            + (1-self.p) * self.cost(self.activation(output),labels)\n",
	"\n",
	"\n",
	"#criterion = nn.CrossEntropyLoss()\n",
	"criterion = customLoss()\n",
	"\n",
	"def Binarize(tensor, include_zero = False, minSig=3):\n",
	"    if include_zero:\n",
	"        P_std = 0.25\n",
	"        up_lim = torch.max(0 + P_std*tensor.std(), torch.ones_like(tensor)*minSig)\n",
	"        down_lim = torch.min(0 - P_std*tensor.std(), -1*torch.ones_like(tensor)*minSig)\n",
	"        up_v = (tensor>up_lim).float()\n",
	"        down_v = (tensor<down_lim).float().mul(-1)\n",
	"        return (up_v + down_v)\n",
	"    else:\n",
	"        return tensor.sign()\n",
	"\n",
	"\"\"\"def Binarize(tensor, include_zero = True):\n",
	"        if include_zero:\n",
	"            return ((tensor+0.5).sign()+(tensor-0.5).sign())/2\n",
	"        else:\n",
	"            return tensor.sign()\"\"\"\n",
	"'''\n",
	"class PositiveBinarizeLinear(nn.Linear):\n",
	"\n",
	"        def __init__(self, *kargs, **kwargs):\n",
	"            super(PositiveBinarizeLinear, self).__init__(*kargs, **kwargs)\n",
	"    \n",
	"        def forward(self, input):\n",
	"            \n",
	"            if input.size(1) != 784:\n",
	"                input.data=Binarize(input.data)\n",
	"                input.data = input.data.add(1).div(2)\n",
	"            #zero = torch.zeros_like(input.data)\n",
	"            #input.data = torch.where(input.data > 0, input.data, zero)\n",
	"            input.data=Binarize(input.data)\n",
	"            if not hasattr(self.weight,'org'):\n",
	"                self.weight.org=self.weight.data.clone()\n",
	"            self.weight.data=Binarize(self.weight.org)\n",
	"            out = nn.functional.linear(input, self.weight)\n",
	"            if not self.bias is None:\n",
	"                self.bias.org=self.bias.data.clone()\n",
	"                out += self.bias.view(1, -1).expand_as(out)\n",
	"\n",
	"            return out\n",
	"'''\n",
	"class BinarizeLinear(nn.Linear):\n",
	"\n",
	"    def __init__(self, *kargs, **kwargs):\n",
	"        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
	"\n",
	"    def forward(self, input):\n",
	"\n",
	"        if input.size(1) != 784:\n",
	"            input.data=Binarize(input.data)\n",
	"        if not hasattr(self.weight,'org'):\n",
	"            self.weight.org=self.weight.data.clone()\n",
	"        self.weight.data=Binarize(self.weight.org)\n",
	"        out = nn.functional.linear(input, self.weight)\n",
	"        if not self.bias is None:\n",
	"            self.bias.org=self.bias.data.clone()\n",
	"            out += self.bias.view(1, -1).expand_as(out)\n",
	"\n",
	"        return out\n",
	"\n",
	"# simplified batchnorm with no mean normalization and separate learnable parameters for positive and negatives\n",
	"class myBatchNorm1d(nn.BatchNorm1d):\n",
	"\n",
	"    def __init__(self, size):\n",
	"        super(myBatchNorm1d, self).__init__(size)\n",
	"        self.eps = 1e-5\n",
	"        self.l1 = nn.Parameter(torch.ones(size))\n",
	"        self.l2 = nn.Parameter(torch.ones(size))\n",
	"        self.running_var = torch.ones(size)\n",
	"        self.momentum = 0.1\n",
	"\n",
	"    def forward(self, input):\n",
	"        device = input.get_device()\n",
	"        if input.size()[0] != 1:\n",
	"            self.running_var = (1-self.momentum) * self.running_var + self.momentum * torch.var(input, keepdim=True, dim=0)\n",
	"            bottom = torch.sqrt(torch.var(input, keepdim=True, dim=0) + self.eps)\n",
	"        else:\n",
	"            bottom = torch.sqrt(self.running_var + self.eps)\n",
	"\n",
	"        top = input * torch.sigmoid(10 * input) * self.l1 + input * torch.sigmoid(-10 * input) * self.l2\n",
	"        \n",
	"        out = top/bottom\n",
	"\n",
	"        return out\n",
	"\n",
	"class Net(nn.Module):\n",
	"    def __init__(self):\n",
	"        super(Net, self).__init__()\n",
	"        self.infl_ratio=3\n",
	"        self.fc1 = BinarizeLinear(784, 1024*self.infl_ratio, bias=False)\n",
	"        self.htanh1 = nn.Hardtanh()\n",
	"        self.bn1 = myBatchNorm1d(1024*self.infl_ratio)\n",
	"        self.fc2 = BinarizeLinear(1024*self.infl_ratio, 1024*self.infl_ratio, bias=False)\n",
	"        self.htanh2 = nn.Hardtanh()\n",
	"        self.bn2 = myBatchNorm1d(1024*self.infl_ratio)\n",
	"        self.fc3 = BinarizeLinear(1024*self.infl_ratio, 1024*self.infl_ratio, bias=False)\n",
	"        self.htanh3 = nn.Hardtanh()\n",
	"        self.bn3 = myBatchNorm1d(1024*self.infl_ratio)\n",
	"        self.fc4 = BinarizeLinear(1024*self.infl_ratio, 10, bias=False)\n",
	"        self.logsoftmax=nn.LogSoftmax()\n",
	"        self.drop=nn.Dropout(0.5)\n",
	"\n",
	"    def forward(self, x):\n",
	"        x = x.view(-1, 28*28)\n",
	"        x = self.fc1(x)\n",
	"        x = self.bn1(x)\n",
	"        x = self.htanh1(x)\n",
	"        x = self.fc2(x)\n",
	"        x = self.bn2(x)\n",
	"        x = self.htanh2(x)\n",
	"        x = self.fc3(x) \n",
	"        x = self.drop(x)\n",
	"        x = self.bn3(x)\n",
	"        x = self.htanh3(x)\n",
	"        x = self.fc4(x)\n",
	"        return x#self.logsoftmax(x)\n",
	"\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"trained_models/CNN_loss_margin_test.pt\",map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "res, model_act = sim.testMNIST(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, sum = sim.testSim(res, model_act,4,1,4)\n",
    "pprint(parse)\n",
    "print(\"sum:\",sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
