{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import modules.model2sim as sim\n",
    "\n",
    "import torch\n",
	"import torch.nn as nn\n",
	"import numpy as np\n",
	"\n",
	"class customLoss(nn.Module):\n",
	"    def __init__(self, p=0.5):\n",
	"        super().__init__()\n",
	"        self.p = p\n",
	"        self.cost = nn.CrossEntropyLoss()\n",
	"        self.cost2 = nn.L1Loss()\n",
	"\n",
	"    def forward(self, output, labels, scale_true=10, scale_wrong=-10):\n",
	"        # scale_wrong = max(sum(output[0])/len(output[0]), 2) * scale\n",
	"        # scale_true = scale_wrong * 10\n",
	"        tempOut = torch.where(output>0,output*scale_wrong,torch.zeros_like(output))\n",
	"        tempLabels = torch.ones_like(output) * scale_wrong\n",
	"        for i in range(labels.size(0)):\n",
	"            j = labels[i]\n",
	"            #tempLabels[i,j] = scale_true \n",
	"            tempLabels[i,j] = torch.max(torch.abs(torch.mean(output[i])), torch.abs(output[i,j]))*scale_true\n",
	"        #output_softmax = nn.LogSoftmax()(output)\n",
	"        return self.p    * self.cost2(tempOut, tempLabels) \\\n",
	"            + (1-self.p) * self.cost(output-torch.ones_like(output),labels)\n",
	"\n",
	"\n",
	"#criterion = nn.CrossEntropyLoss()\n",
	"criterion = customLoss()\n",
	"\n",
	"def Binarize(tensor, include_zero = False, minSig=3):\n",
	"    if include_zero:\n",
	"        P_std = 0.25\n",
	"        up_lim = torch.max(0 + P_std*tensor.std(), torch.ones_like(tensor)*minSig)\n",
	"        down_lim = torch.min(0 - P_std*tensor.std(), -1*torch.ones_like(tensor)*minSig)\n",
	"        up_v = (tensor>up_lim).float()\n",
	"        down_v = (tensor<down_lim).float().mul(-1)\n",
	"        return (up_v + down_v)\n",
	"    else:\n",
	"        return tensor.sign()\n",
	"\n",
	"\"\"\"def Binarize(tensor, include_zero = True):\n",
	"        if include_zero:\n",
	"            return ((tensor+0.5).sign()+(tensor-0.5).sign())/2\n",
	"        else:\n",
	"            return tensor.sign()\"\"\"\n",
	"\n",
	"class PositiveBinarizeLinear(nn.Linear):\n",
	"\n",
	"        def __init__(self, *kargs, **kwargs):\n",
	"            super(PositiveBinarizeLinear, self).__init__(*kargs, **kwargs)\n",
	"    \n",
	"        def forward(self, input):\n",
	"            \n",
	"            if input.size(1) != 784:\n",
	"                input.data=Binarize(input.data)\n",
	"                input.data = input.data.add(1).div(2)\n",
	"            #zero = torch.zeros_like(input.data)\n",
	"            #input.data = torch.where(input.data > 0, input.data, zero)\n",
	"            input.data=Binarize(input.data)\n",
	"            if not hasattr(self.weight,'org'):\n",
	"                self.weight.org=self.weight.data.clone()\n",
	"            self.weight.data=Binarize(self.weight.org)\n",
	"            out = nn.functional.linear(input, self.weight)\n",
	"            if not self.bias is None:\n",
	"                self.bias.org=self.bias.data.clone()\n",
	"                out += self.bias.view(1, -1).expand_as(out)\n",
	"\n",
	"            return out\n",
	"\n",
	"class BinarizeLinear(nn.Linear):\n",
	"\n",
	"    def __init__(self, *kargs, **kwargs):\n",
	"        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
	"\n",
	"    def forward(self, input):\n",
	"\n",
	"        if input.size(1) != 784:\n",
	"            input.data=Binarize(input.data)\n",
	"        if not hasattr(self.weight,'org'):\n",
	"            self.weight.org=self.weight.data.clone()\n",
	"        self.weight.data=Binarize(self.weight.org)\n",
	"        out = nn.functional.linear(input, self.weight)\n",
	"        if not self.bias is None:\n",
	"            self.bias.org=self.bias.data.clone()\n",
	"            out += self.bias.view(1, -1).expand_as(out)\n",
	"\n",
	"        return out\n",
	"\n",
	"class Net(nn.Module):\n",
	"    def __init__(self):\n",
	"        super(Net, self).__init__()\n",
	"        self.fc1 = BinarizeLinear(784, 1000, bias=False)\n",
	"        self.htanh01 = nn.Hardtanh()\n",
	"        self.fcp2 = BinarizeLinear(1000, 500, bias=False)\n",
	"        self.htanh02 = nn.Hardtanh()\n",
	"        self.drop = nn.Dropout(p=0.4)\n",
	"        self.fcp3 = BinarizeLinear(500, 100, bias=False)\n",
	"        self.htanh03 = nn.Hardtanh()\n",
	"        self.fc5 = BinarizeLinear(100, 10, bias=False)\n",
	"        self.htanh04 = nn.Hardtanh()\n",
	"\n",
	"    def forward(self, x):\n",
	"        x = x.view(-1, 28*28)\n",
	"        x = self.fc1(x)\n",
	"        x = self.htanh01(x)\n",
	"        x = self.fcp2(x)\n",
	"        x = self.htanh02(x)\n",
	"        x = self.drop(x)\n",
	"        x = self.fcp3(x)\n",
	"        x = self.htanh03(x)\n",
	"        x = self.fc5(x)\n",
	"        x = self.htanh04(x)\n",
	"        return x\n",
	"\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"trained_models/test-big-pos.pt\",map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "res, model_act = sim.testMNIST(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, sum = sim.testSim(res, model_act,4,1,4)\n",
    "pprint(parse)\n",
    "print(\"sum:\",sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
