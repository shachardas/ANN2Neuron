{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Device index must not be negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d12ff0eedbf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trained_models/original_customloss.pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nestDocker/ANN2Neuron/clean/simulation/modules/model2sim.py\u001b[0m in \u001b[0;36mtestMNIST\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ANN result:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanSamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimfromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d12ff0eedbf5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtanh1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d12ff0eedbf5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Device index must not be negative"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import modules.model2sim as sim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class customLoss(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "        self.cost2 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, output, labels, scale_true=10, scale_wrong=-10):\n",
    "        # scale_wrong = max(sum(output[0])/len(output[0]), 2) * scale\n",
    "        # scale_true = scale_wrong * 10\n",
    "        tempOut = torch.where(output>0,output*scale_wrong,torch.zeros_like(output))\n",
    "        tempLabels = torch.ones_like(output) * scale_wrong\n",
    "        for i in range(labels.size(0)):\n",
    "            j = labels[i]\n",
    "            #tempLabels[i,j] = scale_true \n",
    "            tempLabels[i,j] = torch.max(torch.abs(torch.mean(output[i])), torch.abs(output[i,j]))*scale_true\n",
    "        #output_softmax = nn.LogSoftmax()(output)\n",
    "        return self.p    * self.cost2(tempOut, tempLabels) \\\n",
    "            + (1-self.p) * self.cost(output,labels)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = customLoss()\n",
    "\n",
    "def Binarize(tensor, include_zero = False, minSig=3):\n",
    "    if include_zero:\n",
    "        P_std = 0.25\n",
    "        up_lim = torch.max(0 + P_std*tensor.std(), torch.ones_like(tensor)*minSig)\n",
    "        down_lim = torch.min(0 - P_std*tensor.std(), -1*torch.ones_like(tensor)*minSig)\n",
    "        up_v = (tensor>up_lim).float()\n",
    "        down_v = (tensor<down_lim).float().mul(-1)\n",
    "        return (up_v + down_v)\n",
    "    else:\n",
    "        return tensor.sign()\n",
    "\n",
    "\"\"\"def Binarize(tensor, include_zero = True):\n",
    "        if include_zero:\n",
    "            return ((tensor+0.5).sign()+(tensor-0.5).sign())/2\n",
    "        else:\n",
    "            return tensor.sign()\"\"\"\n",
    "\n",
    "class PositiveBinarizeLinear(nn.Linear):\n",
    "\n",
    "        def __init__(self, *kargs, **kwargs):\n",
    "            super(PositiveBinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "    \n",
    "        def forward(self, input):\n",
    "            \n",
    "            if input.size(1) != 784:\n",
    "                input.data=Binarize(input.data)\n",
    "                input.data = input.data.add(1).div(2)\n",
    "            #zero = torch.zeros_like(input.data)\n",
    "            #input.data = torch.where(input.data > 0, input.data, zero)\n",
    "            input.data=Binarize(input.data)\n",
    "            if not hasattr(self.weight,'org'):\n",
    "                self.weight.org=self.weight.data.clone()\n",
    "            self.weight.data=Binarize(self.weight.org)\n",
    "            out = nn.functional.linear(input, self.weight)\n",
    "            if not self.bias is None:\n",
    "                self.bias.org=self.bias.data.clone()\n",
    "                out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "            return out\n",
    "\n",
    "class BinarizeLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if input.size(1) != 784:\n",
    "            input.data=Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "        out = nn.functional.linear(input, self.weight)\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# simplified batchnorm with no mean normalization and separate learnable parameters for positive and negatives\n",
    "class myBatchNorm1d(nn.BatchNorm1d):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        super(myBatchNorm1d, self).__init__(size)\n",
    "        self.eps = 1e-5\n",
    "        self.l1 = nn.Parameter(torch.ones(size))\n",
    "        self.l2 = nn.Parameter(torch.ones(size))\n",
    "        self.running_var = torch.ones(size)\n",
    "        self.momentum = 0.1\n",
    "\n",
    "    def forward(self, input):\n",
    "        device = input.get_device()\n",
    "        if input.dim() == 2:\n",
    "            self.running_var = (1-self.momentum) * self.running_var + self.momentum * torch.var(input, keepdim=True, dim=0)\n",
    "            bottom = torch.sqrt(torch.var(input, keepdim=True, dim=0) + self.eps)\n",
    "        else:\n",
    "            bottom = torch.sqrt(self.running_var + self.eps)\n",
    "\n",
    "        top = input * torch.sigmoid(10 * input) * self.l1.to(device) + input * torch.sigmoid(-10 * input) * self.l2.to(device)\n",
    "        \n",
    "\n",
    "        out = top/bottom\n",
    "\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.infl_ratio=3\n",
    "        self.fc1 = BinarizeLinear(784, 1024*self.infl_ratio, bias=False)\n",
    "        self.htanh1 = nn.Hardtanh()\n",
    "        self.bn1 = myBatchNorm1d(1024*self.infl_ratio)\n",
    "        self.fc2 = BinarizeLinear(1024*self.infl_ratio, 1024*self.infl_ratio, bias=False)\n",
    "        self.htanh2 = nn.Hardtanh()\n",
    "        self.bn2 = myBatchNorm1d(1024*self.infl_ratio)\n",
    "        self.fc3 = BinarizeLinear(1024*self.infl_ratio, 1024*self.infl_ratio, bias=False)\n",
    "        self.htanh3 = nn.Hardtanh()\n",
    "        self.bn3 = myBatchNorm1d(1024*self.infl_ratio)\n",
    "        self.fc4 = nn.Linear(1024*self.infl_ratio, 10, bias=False)\n",
    "        self.logsoftmax=nn.LogSoftmax()\n",
    "        self.drop=nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.htanh1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.htanh2(x)\n",
    "        x = self.fc3(x) \n",
    "        x = self.drop(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.htanh3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x#self.logsoftmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"trained_models/original_customloss.pt\",map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "res, model_act = sim.testMNIST(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, sum = sim.testSim(res, model_act,4,1,4)\n",
    "pprint(parse)\n",
    "print(\"sum:\",sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
